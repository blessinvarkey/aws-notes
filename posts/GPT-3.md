# GPT-3 | Generative Pre-trained Transformer 3 

- GPT-3 is a language prediction model by OpenAI.  
- An example of **unsupervised learning** as the training data doesn't include information that is right or wrong.      
- To learn how to build language constructs, such as sentences, it employs semantic analytics - studying not just the words and their meanings, but also gathering an understanding of how the usage of words differs depending on other words also used in the text.   
- It's an AI that is better at creating content that has a language structure – human or machine language – than anything that has come before it.    
- GPT or Generative Pre-trained Transformer (-3) generates text using algorithms that are pre-trained or pre fed with the required data.     
- Data has been collected from [Common Crawl](https://commoncrawl.org), Wikipedia and other sources.      
- It scans all the text in its training data – hundreds of billions of words, arranged into meaningful language – and determines what word it should use to recreate the original phrase.   
- GPT-3 can answer questions, write essays, summarize long texts, translate languages, take memos, and even create computer code     


